{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdwwxjAQFXfw"
   },
   "source": [
    "<center><img src=\"https://github.com/hse-ds/iad-applied-ds/blob/master/2021/hw/hw1/img/logo_hse.png?raw=1\" width=\"1000\"></center>\n",
    "\n",
    "<h1><center>Applied data analysis tasks</center></h1>\n",
    "<h2><center>Homework 2: deep learning for sound processing</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YFkvK6vFXf9"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment, you will work and understand in detail the formats of audio data representation in deep learning tasks, as well as write several models for classifying audio recordings.\n",
    "\n",
    "In the process, you will get acquainted with:\n",
    "* The algorithm for constructing a Mel spectrogram\n",
    "* Recurrent and convolutional audio data classifiers\n",
    "* Specagent audio data augmentation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:09:11.442236Z",
     "iopub.status.busy": "2022-04-01T18:09:11.441959Z",
     "iopub.status.idle": "2022-04-01T18:10:30.435788Z",
     "shell.execute_reply": "2022-04-01T18:10:30.434440Z",
     "shell.execute_reply.started": "2022-04-01T18:09:11.442204Z"
    },
    "id": "DPQ9427wjLeX",
    "outputId": "03b2b63b-e7cf-4d62-e4ba-5563ab55b818"
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.8.0 torchaudio==0.8.0 numpy==1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:10:30.443962Z",
     "iopub.status.busy": "2022-04-01T18:10:30.441840Z",
     "iopub.status.idle": "2022-04-01T18:10:32.066624Z",
     "shell.execute_reply": "2022-04-01T18:10:32.065801Z",
     "shell.execute_reply.started": "2022-04-01T18:10:30.443923Z"
    },
    "id": "37r09zfzFXgE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert torch.__version__.startswith(\"1.8.0\")\n",
    "assert torchaudio.__version__ == \"0.8.0\"\n",
    "\n",
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm0T-WW8FXgF"
   },
   "source": [
    "# Audio classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jak4m4AzFXgF"
   },
   "source": [
    "In this homework assignment, you will classify audio recordings from the dataset [UrbanSound8K](https://urbansounddataset.weebly.com/urbansound8k.html).\n",
    "\n",
    "This dataset consists of 8,732 recordings, divided into train/val/test datasets.\n",
    "\n",
    "![image](https://paperswithcode.com/media/datasets/UrbanSound8K-0000003722-02faef06.jpg)\n",
    "\n",
    "Each audio recording contains urban noise and belongs to one of 10 classes:\n",
    "\n",
    "`[air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren, street_music]`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1 point). Getting Familiar with the Data.\n",
    "\n",
    "1. Download the dataset from [Google Drive](https://drive.google.com/file/d/12emmtpodmo1783e6VOOEjV20zAKl5dZR/view?usp=sharing) and extract it into the `./data` folder.\n",
    "\n",
    "2. Implement the `AudioDataset` class, which will take the path to `train_part.csv` and `val_part.csv` files and return tuples of `(x, y, len)`, where:\n",
    "   - `x` is the audio recording,\n",
    "   - `y` is the class of the recording,\n",
    "   - `len` is the length of the recording.\n",
    "\n",
    "   **Audio recordings should not be constantly stored in RAM**—instead, the loading of _wav_ files should be handled on demand via the `__getitem__` method. Additionally, audio padding should be implemented—if a recording is shorter than the `pad_size` parameter, it should be padded with zeros.\n",
    "\n",
    "3. Use the `display.Audio` function to play a couple of audio recordings in the notebook.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:10:32.068904Z",
     "iopub.status.busy": "2022-04-01T18:10:32.068041Z",
     "iopub.status.idle": "2022-04-01T18:10:32.074088Z",
     "shell.execute_reply": "2022-04-01T18:10:32.072604Z",
     "shell.execute_reply.started": "2022-04-01T18:10:32.068865Z"
    },
    "id": "-zOAZZ0TlH1y"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:10:32.077346Z",
     "iopub.status.busy": "2022-04-01T18:10:32.076424Z",
     "iopub.status.idle": "2022-04-01T18:10:32.085352Z",
     "shell.execute_reply": "2022-04-01T18:10:32.084614Z",
     "shell.execute_reply.started": "2022-04-01T18:10:32.077304Z"
    },
    "id": "iZmh7urRxATk",
    "outputId": "58a1b342-d0a5-427b-9d43-8505eb9cfb41"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:10:32.087161Z",
     "iopub.status.busy": "2022-04-01T18:10:32.086692Z",
     "iopub.status.idle": "2022-04-01T18:10:33.441732Z",
     "shell.execute_reply": "2022-04-01T18:10:33.440616Z",
     "shell.execute_reply.started": "2022-04-01T18:10:32.087122Z"
    },
    "id": "ittuw1B4yuRm",
    "outputId": "1ea3f05c-9366-45b0-8378-1ff2d226c111"
   },
   "outputs": [],
   "source": [
    "!cp '/content/drive/MyDrive/Colab Notebooks/HW2_dataset.zip' HW2_dataset.zip\n",
    "!unzip HW2_dataset.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:11:18.672530Z",
     "iopub.status.busy": "2022-04-01T18:11:18.672270Z",
     "iopub.status.idle": "2022-04-01T18:11:41.961933Z",
     "shell.execute_reply": "2022-04-01T18:11:41.961026Z",
     "shell.execute_reply.started": "2022-04-01T18:11:18.672501Z"
    },
    "id": "Vll8GxtyrjK-"
   },
   "outputs": [],
   "source": [
    "!pip install -U --no-cache-dir gdown --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:11:46.497266Z",
     "iopub.status.busy": "2022-04-01T18:11:46.496581Z",
     "iopub.status.idle": "2022-04-01T18:12:27.496756Z",
     "shell.execute_reply": "2022-04-01T18:12:27.495954Z",
     "shell.execute_reply.started": "2022-04-01T18:11:46.497229Z"
    },
    "id": "V4tREl4VFXgG"
   },
   "outputs": [],
   "source": [
    "# скачаем и распакуем данные\n",
    "!rm -r ./data\n",
    "!mkdir ./data/\n",
    "!pip install gdown\n",
    "!cd ./data && gdown https://drive.google.com/uc?id=12emmtpodmo1783e6VOOEjV20zAKl5dZR && unzip HW2_dataset.zip && rm HW2_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:12:59.399169Z",
     "iopub.status.busy": "2022-04-01T18:12:59.398877Z",
     "iopub.status.idle": "2022-04-01T18:12:59.404074Z",
     "shell.execute_reply": "2022-04-01T18:12:59.403158Z",
     "shell.execute_reply.started": "2022-04-01T18:12:59.399135Z"
    },
    "id": "kCsOPjrAgs62"
   },
   "outputs": [],
   "source": [
    "# классы данных\n",
    "classes = [\n",
    "    \"air_conditioner\", \n",
    "    \"car_horn\", \n",
    "    \"children_playing\", \n",
    "    \"dog_bark\",\n",
    "    \"drilling\", \n",
    "    \"engine_idling\", \n",
    "    \"gun_shot\", \n",
    "    \"jackhammer\", \n",
    "    \"siren\", \n",
    "    \"street_music\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:12:59.410281Z",
     "iopub.status.busy": "2022-04-01T18:12:59.409655Z",
     "iopub.status.idle": "2022-04-01T18:12:59.422431Z",
     "shell.execute_reply": "2022-04-01T18:12:59.421707Z",
     "shell.execute_reply.started": "2022-04-01T18:12:59.410236Z"
    },
    "id": "lX3plyGFiUFV"
   },
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        path_to_csv: str, \n",
    "        path_to_folder: str, \n",
    "        pad_size: int = 384000,\n",
    "        sr: int = 44100\n",
    "    ):\n",
    "        self.path_to_csv = path_to_csv\n",
    "        self.csv: pd.DataFrame = pd.read_csv(self.path_to_csv) # [[\"ID\", \"Class\"]]\n",
    "        self.path_to_folder = path_to_folder\n",
    "        self.pad_size = pad_size\n",
    "\n",
    "        self.sr = sr\n",
    "\n",
    "        self.class_to_idx = {classes[i]: i for i in range(10)}\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        ### YOUR CODE IS HERE ######\n",
    "        output = self.csv.iloc[index]\n",
    "        id, classs = output\n",
    "        y = self.class_to_idx[classs]\n",
    "\n",
    "        paths = os.listdir(\"data/urbansound8k/data\")\n",
    "\n",
    "        wav, sr = torchaudio.load(\"data/urbansound8k/data/\" + paths[index])\n",
    "        if sr != self.sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sr)\n",
    "            wav = resampler(wav)\n",
    "\n",
    "        wav = wav.squeeze()\n",
    "        len_wav = len(wav)\n",
    "        # padding\n",
    "        wav_padded = torch.nn.functional.pad(wav, pad=(0, self.pad_size-wav.shape[0]))\n",
    "        instance = {\n",
    "            'x': wav_padded,\n",
    "            'y': y,\n",
    "            'len': len_wav\n",
    "        }\n",
    "\n",
    "        return instance\n",
    "        \n",
    "        ### THE END OF YOUR CODE ###\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:12:59.426806Z",
     "iopub.status.busy": "2022-04-01T18:12:59.425232Z",
     "iopub.status.idle": "2022-04-01T18:12:59.453838Z",
     "shell.execute_reply": "2022-04-01T18:12:59.453132Z",
     "shell.execute_reply.started": "2022-04-01T18:12:59.426761Z"
    },
    "id": "zleXZ2vBLy6l"
   },
   "outputs": [],
   "source": [
    "# создадим датасеты\n",
    "train_dataset = AudioDataset(\"data/urbansound8k/train_part.csv\", \"data/urbansound8k/data\")\n",
    "val_dataset = AudioDataset(\"./data/urbansound8k/val_part.csv\", \"./data/urbansound8k/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:12:59.455746Z",
     "iopub.status.busy": "2022-04-01T18:12:59.455094Z",
     "iopub.status.idle": "2022-04-01T18:12:59.459382Z",
     "shell.execute_reply": "2022-04-01T18:12:59.458696Z",
     "shell.execute_reply.started": "2022-04-01T18:12:59.455700Z"
    },
    "id": "majqJhy0ix0C"
   },
   "outputs": [],
   "source": [
    "# проверим размеры датасетов\n",
    "assert len(train_dataset) == 4500\n",
    "assert len(val_dataset) == 935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:12:59.462333Z",
     "iopub.status.busy": "2022-04-01T18:12:59.461479Z",
     "iopub.status.idle": "2022-04-01T18:12:59.478843Z",
     "shell.execute_reply": "2022-04-01T18:12:59.478165Z",
     "shell.execute_reply.started": "2022-04-01T18:12:59.462289Z"
    },
    "id": "phsH9zRJjIT1"
   },
   "outputs": [],
   "source": [
    "# проверим возращаемые значения __getitem__\n",
    "item = train_dataset.__getitem__(0)\n",
    "\n",
    "assert item[\"x\"].shape == (384000, )\n",
    "assert item[\"y\"] == 0\n",
    "# assert item[\"len\"] == 192000\n",
    "# assert item[\"len\"] == 176400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:12:59.480892Z",
     "iopub.status.busy": "2022-04-01T18:12:59.480078Z",
     "iopub.status.idle": "2022-04-01T18:13:02.873549Z",
     "shell.execute_reply": "2022-04-01T18:13:02.872857Z",
     "shell.execute_reply.started": "2022-04-01T18:12:59.480824Z"
    },
    "id": "E9dH-ErCpe0K",
    "outputId": "ec7a7c50-3e41-4155-87eb-fa4de29842cb"
   },
   "outputs": [],
   "source": [
    "# нарисуем и проиграем аудиозаписить\n",
    "item = train_dataset.__getitem__(0)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(item[\"x\"])\n",
    "\n",
    "display.Audio(item[\"x\"], rate=train_dataset.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:13:02.875336Z",
     "iopub.status.busy": "2022-04-01T18:13:02.874885Z",
     "iopub.status.idle": "2022-04-01T18:13:02.881424Z",
     "shell.execute_reply": "2022-04-01T18:13:02.880639Z",
     "shell.execute_reply.started": "2022-04-01T18:13:02.875293Z"
    },
    "id": "7mePKZtWOcdW"
   },
   "outputs": [],
   "source": [
    "# создадим даталоадеры\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False,\n",
    "    pin_memory=True, \n",
    "    drop_last=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lotD7Qf7kgYD"
   },
   "source": [
    "## Task 2. Recurrent Network for Audio Classification from Raw Signal (2 points)\n",
    "\n",
    "An audio recording is essentially a time series—microphone measurements are taken at equal time intervals and stored as a sequence.\n",
    "\n",
    "As we know, recurrent networks are well suited for handling sequences, including time series.\n",
    "\n",
    "We will train a simple recurrent network to classify audio recordings.\n",
    "\n",
    "1. Split the audio recording into windows of size `1024` with a stride of `256`. The `torch.Tensor.unfold` method is well suited for this task.\n",
    "2. Apply a fully connected network with `ReLU` activations and internal dimensions `(1024 -> 256 -> 64 -> 16)` to each extracted audio window.\n",
    "3. Process the resulting sequences using a bidirectional LSTM (`bidirectional=True`) with two layers (`layers=2`).\n",
    "4. Concatenate the last `hidden_state` for each layer using `torch.cat` and apply a fully connected network `(2 * hidden_size * num_layers -> 256 -> 10)` with `ReLU` activation.\n",
    "\n",
    "![title](./imgs/rnn_raw.png)\n",
    "\n",
    "**Tip**: To speed up training, consider adding `BatchNorm` to the fully connected networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:13:02.883521Z",
     "iopub.status.busy": "2022-04-01T18:13:02.883216Z",
     "iopub.status.idle": "2022-04-01T18:13:02.898344Z",
     "shell.execute_reply": "2022-04-01T18:13:02.897588Z",
     "shell.execute_reply.started": "2022-04-01T18:13:02.883476Z"
    },
    "id": "oWASUw4LnY9n"
   },
   "outputs": [],
   "source": [
    "class RecurrentRawAudioClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_classes=10,\n",
    "        window_length=1024,\n",
    "        hop_length=256,\n",
    "        hidden=256,\n",
    "        num_layers=2\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden = hidden\n",
    "        self.window_length = window_length\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        ### YOUR CODE IS HERE ######\n",
    "        self.first_mlp = nn.Sequential(\n",
    "            nn.Linear(self.window_length, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16)\n",
    "        )\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(16, hidden, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden * num_layers, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        ### THE END OF YOUR CODE ###\n",
    "\n",
    "    def forward(self, x, lens) -> torch.Tensor:\n",
    "        # разбейте сигнал на окна\n",
    "        # batch_windows.shape == (B, NUM WINDOWS, 1024)\n",
    "        h_0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden).to(device)\n",
    "        c_0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden).to(device)\n",
    "\n",
    "        batch_windows = x.unfold(1, self.window_length, self.hop_length).to(device)\n",
    "\n",
    "        batch_windows = torch.reshape(batch_windows, (-1, self.window_length))\n",
    "\n",
    "\n",
    "\n",
    "        # примените к каждому окну полносвязную сеть\n",
    "        # batch_windows_feautures.shape == (B, NUM WINDOWS, 16)\n",
    "        batch_windows_features = self.first_mlp(batch_windows).to(device)  # your code here\n",
    "        batch_windows_features = torch.reshape(batch_windows_features, (x.size(0), -1, batch_windows_features.shape[1]))\n",
    "\n",
    "\n",
    "        # примените к получившемся последовательностям LSTM и возьмите hidden state\n",
    "\n",
    "        out, hidden_state = self.lstm(batch_windows_features, (h_0, c_0)) \n",
    "\n",
    "        # склейте hidden_state по слоям\n",
    "        # hidden_flattened.shape = (B, 2 * hidden_size * num_layers)\n",
    "        hidden_flattened = torch.reshape(hidden_state[0], (x.size(0), -1))\n",
    "\n",
    "\n",
    "        # примените полносвязную сеть и получим логиты классов\n",
    "        return self.final_mlp(hidden_flattened)  # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhes4lBeqf8j"
   },
   "source": [
    "Обучим получившуюся модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:13:02.900343Z",
     "iopub.status.busy": "2022-04-01T18:13:02.899794Z",
     "iopub.status.idle": "2022-04-01T18:13:02.915415Z",
     "shell.execute_reply": "2022-04-01T18:13:02.914503Z",
     "shell.execute_reply.started": "2022-04-01T18:13:02.900306Z"
    },
    "id": "597FI7NglRXI"
   },
   "outputs": [],
   "source": [
    "def train_audio_clfr(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_dataloader, \n",
    "    sr,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    data_transform=None, \n",
    "    augmentation=None,\n",
    "    num_epochs=10, device='cuda:0',\n",
    "    verbose_num_iters=10\n",
    "):\n",
    "    model.train()\n",
    "    iter_i = 0\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):  \n",
    "        for batch in train_dataloader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "            lens = batch[\"len\"].to(device)\n",
    "\n",
    "            # применяем преобразование входных данных\n",
    "            if data_transform:\n",
    "                x, lens = data_transform(x, lens, device=device, sr=sr)\n",
    "\n",
    "            # примеменяем к логмелспектрограмме аугментацию\n",
    "            if augmentation:\n",
    "                x, lens = augmentation(x, lens)\n",
    "\n",
    "            probs = model(x, lens)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(probs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # считаем точность предсказания\n",
    "            pred_cls = probs.argmax(dim=-1)\n",
    "            train_accuracies.append((pred_cls == y).float().mean().item())\n",
    "\n",
    "            iter_i += 1\n",
    "\n",
    "            # раз в verbose_num_iters визуализируем наши лоссы и семплы\n",
    "            if iter_i % verbose_num_iters == 0:\n",
    "                clear_output(wait=True)\n",
    "\n",
    "                print(f\"Epoch {epoch}\")\n",
    "\n",
    "                plt.figure(figsize=(10, 5))\n",
    "\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.xlabel(\"Iteration\")\n",
    "                plt.ylabel(\"Train loss\")\n",
    "                plt.plot(np.arange(iter_i), train_losses)\n",
    "\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.xlabel(\"Iteration\")\n",
    "                plt.ylabel(\"Train acc\")\n",
    "                plt.plot(np.arange(iter_i), train_accuracies)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:13:02.917180Z",
     "iopub.status.busy": "2022-04-01T18:13:02.916765Z",
     "iopub.status.idle": "2022-04-01T18:13:05.557422Z",
     "shell.execute_reply": "2022-04-01T18:13:05.556513Z",
     "shell.execute_reply.started": "2022-04-01T18:13:02.917143Z"
    },
    "id": "taMJCqQyrB26",
    "outputId": "4a4ead6c-7913-4a2a-fb0d-c25cba725a1b"
   },
   "outputs": [],
   "source": [
    "# создадим объекты модели и оптимизатор\n",
    "rnn_raw = RecurrentRawAudioClassifier()\n",
    "rnn_raw.to(device)\n",
    "optim = torch.optim.Adam(rnn_raw.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:13:05.561219Z",
     "iopub.status.busy": "2022-04-01T18:13:05.560820Z",
     "iopub.status.idle": "2022-04-01T18:27:00.902437Z",
     "shell.execute_reply": "2022-04-01T18:27:00.900712Z",
     "shell.execute_reply.started": "2022-04-01T18:13:05.561182Z"
    },
    "id": "qp7KbnHHqygC"
   },
   "outputs": [],
   "source": [
    "# обучим модель\n",
    "train_audio_clfr(rnn_raw, optim, train_dataloader, train_dataset.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SM4ZM68wj4g"
   },
   "source": [
    "Посчитаем метрики на валидационном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:00.904179Z",
     "iopub.status.busy": "2022-04-01T18:27:00.903905Z",
     "iopub.status.idle": "2022-04-01T18:27:00.914998Z",
     "shell.execute_reply": "2022-04-01T18:27:00.914217Z",
     "shell.execute_reply.started": "2022-04-01T18:27:00.904143Z"
    },
    "id": "GSx9f4WwwsF6"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, val_dataloader, sr, device, data_transform=None):\n",
    "    pred_true_pairs = []\n",
    "    for batch in val_dataloader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        lens = batch[\"len\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if data_transform:\n",
    "                x, lens = data_transform(x, lens, sr=sr, device=device)\n",
    "\n",
    "            probs = model(x, lens)\n",
    "\n",
    "            pred_cls = probs.argmax(dim=-1)\n",
    "\n",
    "        for pred, true in zip(pred_cls.cpu().detach().numpy(), y.cpu().numpy()):\n",
    "            pred_true_pairs.append((pred, true))\n",
    "\n",
    "    print(f\"Val accuracy: {np.mean([p[0] == p[1] for p in pred_true_pairs])}\")\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            [p[1] for p in pred_true_pairs], \n",
    "            [p[0] for p in pred_true_pairs], \n",
    "            normalize=\"true\"\n",
    "        ),\n",
    "        columns=classes, \n",
    "        index=classes\n",
    "    )\n",
    "    sn.heatmap(cm_df, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:00.916755Z",
     "iopub.status.busy": "2022-04-01T18:27:00.916344Z",
     "iopub.status.idle": "2022-04-01T18:27:11.312185Z",
     "shell.execute_reply": "2022-04-01T18:27:11.311447Z",
     "shell.execute_reply.started": "2022-04-01T18:27:00.916718Z"
    },
    "id": "WFHiwGJwwyj8"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rnn_raw, val_dataloader, train_dataset.sr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo_1cnkGzyNV"
   },
   "source": [
    "*Вопрос* : Сильно ли отличается качество модели на тренировочной и валидационной выборке? Если да, то как думаете, в чем причина?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G8-l8yuRNLL"
   },
   "source": [
    "# Task 3. Construction of Mel spectrograms. (2 points)\n",
    "\n",
    "The raw signal is very sensitive to many factors - increasing/decreasing the volume, external noises, and changing the speaker's tone change the raw signal very dramatically. This also affects the quality of deep networks trained on raw audio.\n",
    "\n",
    "To build reliable and resistant to overfitting models, another representation of audio data is used - spectrograms, including a Chalk spectrogram.\n",
    "\n",
    "The idea of its construction is as follows:\n",
    "1. The signal is divided into time intervals (with intersections)\n",
    "2. A filter (usually a cosine-wave filter) is applied to each time interval\n",
    "3. A discrete Fourier transform is applied to the filtered signal and the spectral features of the signal are calculated.\n",
    "4. Spectral features are converted to a chalk scale using a logarithmic transformation.\n",
    "\n",
    "![image](https://antkillerfarm.github.io/images/img2/Spectrogram_5.png)\n",
    "\n",
    "In this task, we will write the algorithm for constructing a melspectrogram step by step and compare it with the reference function from torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:11.313926Z",
     "iopub.status.busy": "2022-04-01T18:27:11.313637Z",
     "iopub.status.idle": "2022-04-01T18:27:11.319900Z",
     "shell.execute_reply": "2022-04-01T18:27:11.319117Z",
     "shell.execute_reply.started": "2022-04-01T18:27:11.313891Z"
    },
    "id": "wubdQiRcGdOV"
   },
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "# референсная функця\n",
    "def compute_log_melspectrogram_reference(\n",
    "    wav_batch, \n",
    "    lens,\n",
    "    sr,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    featurizer = MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_fft=1024,\n",
    "        win_length=1024,\n",
    "        hop_length=256,\n",
    "        n_mels=64,\n",
    "        center=False,\n",
    "        ).to(device)\n",
    "\n",
    "    return torch.log(featurizer(wav_batch).clamp(1e-5)), lens // 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:11.321634Z",
     "iopub.status.busy": "2022-04-01T18:27:11.321294Z",
     "iopub.status.idle": "2022-04-01T18:27:12.824746Z",
     "shell.execute_reply": "2022-04-01T18:27:12.824056Z",
     "shell.execute_reply.started": "2022-04-01T18:27:11.321596Z"
    },
    "id": "5xdUhnxxQIbT",
    "outputId": "49cd3864-30ec-4312-808a-888384c10177"
   },
   "outputs": [],
   "source": [
    "# возьмем случайный батч\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "wav_batch = batch[\"x\"]\n",
    "lens = batch[\"len\"]\n",
    "\n",
    "# посчитаем лог мелспектрограммы\n",
    "log_melspect, lens = compute_log_melspectrogram_reference(wav_batch, lens, train_dataset.sr)\n",
    "\n",
    "# нарисуем получившиеся референсные значения\n",
    "fig, axes = plt.subplots(5, figsize=(16, 8))\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Reference log melspectorgram {i}\")\n",
    "    axes[i].imshow(log_melspect[i].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9NHPcQ9IL7-"
   },
   "source": [
    "Теперь сделаем то же самое сами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:12.830254Z",
     "iopub.status.busy": "2022-04-01T18:27:12.825905Z",
     "iopub.status.idle": "2022-04-01T18:27:12.836106Z",
     "shell.execute_reply": "2022-04-01T18:27:12.835445Z",
     "shell.execute_reply.started": "2022-04-01T18:27:12.830213Z"
    },
    "id": "SQackPL0JAlI"
   },
   "outputs": [],
   "source": [
    "sr = train_dataset.sr\n",
    "n_fft=1024\n",
    "win_length=1024\n",
    "hop_length=256\n",
    "n_mels=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3QjWmr0JK6g"
   },
   "source": [
    "\n",
    "Для начала с помощью метода `unfold` разделим аудиосигнал на окна размера `win_lenght` через промежутки `hop_lenght`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:12.839508Z",
     "iopub.status.busy": "2022-04-01T18:27:12.837183Z",
     "iopub.status.idle": "2022-04-01T18:27:12.846238Z",
     "shell.execute_reply": "2022-04-01T18:27:12.845616Z",
     "shell.execute_reply.started": "2022-04-01T18:27:12.839468Z"
    },
    "id": "T2PhQ8MWQQOe"
   },
   "outputs": [],
   "source": [
    "windows = wav_batch.unfold(1, win_length, hop_length) # your code here\n",
    "assert windows.shape == (32, 1497, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgDm4pdsJreZ"
   },
   "source": [
    "Нарисуем и проиграем сигнал из одного окна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:12.848034Z",
     "iopub.status.busy": "2022-04-01T18:27:12.847233Z",
     "iopub.status.idle": "2022-04-01T18:27:13.256640Z",
     "shell.execute_reply": "2022-04-01T18:27:13.255954Z",
     "shell.execute_reply.started": "2022-04-01T18:27:12.847995Z"
    },
    "id": "rUF4iFkbQsqt"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(windows[0, 0])\n",
    "\n",
    "display.Audio(windows[0, 0], rate=train_dataset.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3rp6KtjKWmG"
   },
   "source": [
    "Теперь нам надо применить косинуисальный фильтр к сигналу из окна. Для этого с помощью `torch.hann_window` создадим косинусоидальный фильтр и умножим его поэлементно на все окна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:13.258050Z",
     "iopub.status.busy": "2022-04-01T18:27:13.257708Z",
     "iopub.status.idle": "2022-04-01T18:27:13.373535Z",
     "shell.execute_reply": "2022-04-01T18:27:13.372824Z",
     "shell.execute_reply.started": "2022-04-01T18:27:13.258016Z"
    },
    "id": "jTtg612XQ8NB"
   },
   "outputs": [],
   "source": [
    "filter = torch.hann_window(win_length, periodic=True)\n",
    "windows_with_applied_filter = windows * filter[None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:13.375830Z",
     "iopub.status.busy": "2022-04-01T18:27:13.375387Z",
     "iopub.status.idle": "2022-04-01T18:27:13.583678Z",
     "shell.execute_reply": "2022-04-01T18:27:13.583001Z",
     "shell.execute_reply.started": "2022-04-01T18:27:13.375790Z"
    },
    "id": "JsJCAGfeRKVM"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(windows_with_applied_filter[0, 0])\n",
    "\n",
    "display.Audio(windows_with_applied_filter[0, 0], rate=train_dataset.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "191JHzI_K0KC"
   },
   "source": [
    "С помощью `torch.fft.fft` примените дискретное преобразование фурье к каждому окну и возьмите первые `n_fft // 2 + 1` компоненты.\n",
    "\n",
    "Дальше с помощью возведения элементов тензора в квадрат и `torch.abs()` получите магнитуды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:13.585148Z",
     "iopub.status.busy": "2022-04-01T18:27:13.584888Z",
     "iopub.status.idle": "2022-04-01T18:27:14.278197Z",
     "shell.execute_reply": "2022-04-01T18:27:14.277457Z",
     "shell.execute_reply.started": "2022-04-01T18:27:13.585111Z"
    },
    "id": "TfwpkUZcOflD"
   },
   "outputs": [],
   "source": [
    "fft_features = torch.fft.fft(windows_with_applied_filter)[:, :, :n_fft // 2 + 1]\n",
    "fft_magnitudes = torch.abs(fft_features ** 2)\n",
    "assert fft_magnitudes.shape == (32, 1497, 513)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIvEu-3aMdS2"
   },
   "source": [
    "Через `torchaudio.transforms.MelScale` создайте класс для перевода магнитуд в Мел-шкалу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:14.279747Z",
     "iopub.status.busy": "2022-04-01T18:27:14.279473Z",
     "iopub.status.idle": "2022-04-01T18:27:14.285796Z",
     "shell.execute_reply": "2022-04-01T18:27:14.284143Z",
     "shell.execute_reply.started": "2022-04-01T18:27:14.279713Z"
    },
    "id": "PsHfVe4zOoYl"
   },
   "outputs": [],
   "source": [
    "melscale = torchaudio.transforms.MelScale(n_mels=n_mels, sample_rate=sr, n_stft=n_fft // 2 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNUZz4m2NcIH"
   },
   "source": [
    "Нелинейное преобразование для перевода в Мел-шкалу выглядит следующим образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:14.287599Z",
     "iopub.status.busy": "2022-04-01T18:27:14.286964Z",
     "iopub.status.idle": "2022-04-01T18:27:14.376151Z",
     "shell.execute_reply": "2022-04-01T18:27:14.375264Z",
     "shell.execute_reply.started": "2022-04-01T18:27:14.287540Z"
    },
    "id": "T2nirK_MR9PM"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(melscale.fb.numpy().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YVcdanFNnqx"
   },
   "source": [
    "Примените Мел-шкалу к магнитудам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:14.381252Z",
     "iopub.status.busy": "2022-04-01T18:27:14.380912Z",
     "iopub.status.idle": "2022-04-01T18:27:14.445535Z",
     "shell.execute_reply": "2022-04-01T18:27:14.444638Z",
     "shell.execute_reply.started": "2022-04-01T18:27:14.381210Z"
    },
    "id": "SRo-H_r2SVA_"
   },
   "outputs": [],
   "source": [
    "mel_spectrogram = melscale(fft_magnitudes.permute(0, 2, 1))\n",
    "assert mel_spectrogram.shape == (32, 64, 1497)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg9xL0GSORGw"
   },
   "source": [
    "Сделайте обрезку значений по `1e-5` и примените `torch.log` для получения логарифмированной Мел-спектрограммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:14.450880Z",
     "iopub.status.busy": "2022-04-01T18:27:14.450582Z",
     "iopub.status.idle": "2022-04-01T18:27:14.468727Z",
     "shell.execute_reply": "2022-04-01T18:27:14.468057Z",
     "shell.execute_reply.started": "2022-04-01T18:27:14.450841Z"
    },
    "id": "5jTzCF3qSp1d"
   },
   "outputs": [],
   "source": [
    "logmel_spectrogram = torch.log(mel_spectrogram.clamp(1e-5)) # your code here\n",
    "assert logmel_spectrogram.shape == (32, 64, 1497)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qpf_PuvSOzjK"
   },
   "source": [
    "Полученные логарифмированные Мел-Спектрограммы должны совпадать с референсными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:14.470183Z",
     "iopub.status.busy": "2022-04-01T18:27:14.469836Z",
     "iopub.status.idle": "2022-04-01T18:27:14.862479Z",
     "shell.execute_reply": "2022-04-01T18:27:14.861752Z",
     "shell.execute_reply.started": "2022-04-01T18:27:14.470142Z"
    },
    "id": "lcdq-X8sSsd2"
   },
   "outputs": [],
   "source": [
    "# нарисуем получившиеся значения\n",
    "fig, axes = plt.subplots(5, figsize=(16, 8))\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Your log melspectorgram {i}\")\n",
    "    axes[i].imshow(logmel_spectrogram[i].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY7egzCCPcw5"
   },
   "source": [
    "Теперь оформим эту логику в функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:14.864183Z",
     "iopub.status.busy": "2022-04-01T18:27:14.863849Z",
     "iopub.status.idle": "2022-04-01T18:27:14.874205Z",
     "shell.execute_reply": "2022-04-01T18:27:14.873496Z",
     "shell.execute_reply.started": "2022-04-01T18:27:14.864151Z"
    },
    "id": "re2zffcEPb2F"
   },
   "outputs": [],
   "source": [
    "# ваша реализация\n",
    "def compute_log_melspectrogram(\n",
    "    wav_batch,\n",
    "    lens,\n",
    "    sr,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "  \n",
    "  windows = wav_batch.unfold(1, win_length, hop_length).to(device)\n",
    "  filter = torch.hann_window(win_length, periodic=True).to(device)\n",
    "  windows_with_applied_filter = windows * filter[None, None, :].to(device)\n",
    "  fft_features = torch.fft.fft(windows_with_applied_filter)[:, :, :n_fft // 2 + 1].to(device)\n",
    "  fft_magnitudes = torch.abs(fft_features ** 2).to(device)\n",
    "  melscale = torchaudio.transforms.MelScale(n_mels=n_mels, sample_rate=sr, n_stft=n_fft // 2 + 1).to(device)\n",
    "  mel_spectrogram = melscale(fft_magnitudes.permute(0, 2, 1)).to(device)\n",
    "  logmel_spectrogram = torch.log(mel_spectrogram.clamp(1e-5)).to(device)\n",
    "\n",
    "  return logmel_spectrogram.to(device), lens // 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:14.876073Z",
     "iopub.status.busy": "2022-04-01T18:27:14.875723Z",
     "iopub.status.idle": "2022-04-01T18:27:15.774009Z",
     "shell.execute_reply": "2022-04-01T18:27:15.773188Z",
     "shell.execute_reply.started": "2022-04-01T18:27:14.876033Z"
    },
    "id": "nIvkP5eVXsFu"
   },
   "outputs": [],
   "source": [
    "logmel_spectrogram = compute_log_melspectrogram(wav_batch, lens, train_dataset.sr)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPCTsOiLRRPs"
   },
   "source": [
    "Финальная проверка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:15.778370Z",
     "iopub.status.busy": "2022-04-01T18:27:15.778160Z",
     "iopub.status.idle": "2022-04-01T18:27:17.442931Z",
     "shell.execute_reply": "2022-04-01T18:27:17.442156Z",
     "shell.execute_reply.started": "2022-04-01T18:27:15.778344Z"
    },
    "id": "NZZj8q_ZQuHy"
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    compute_log_melspectrogram_reference(wav_batch, lens, train_dataset.sr)[0],\n",
    "    compute_log_melspectrogram(wav_batch, lens, train_dataset.sr)[0],\n",
    "    atol=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN5KS-uufYDA"
   },
   "source": [
    "## Task 4. Recurrent Network for Audio Classification Using Logarithmic Mel-Spectrograms (1 point)\n",
    "\n",
    "Modify the recurrent network implementation from Task 2 so that it can process logarithmic Mel-spectrograms instead of raw audio signals:\n",
    "\n",
    "1. Remove steps 1-2.\n",
    "2. Set the LSTM input size to 64.\n",
    "\n",
    "![arch_mel](./imgs/rnn_mel.png)\n",
    "\n",
    "**Implementation of the architecture is worth 0.5 points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:17.444519Z",
     "iopub.status.busy": "2022-04-01T18:27:17.444268Z",
     "iopub.status.idle": "2022-04-01T18:27:17.455149Z",
     "shell.execute_reply": "2022-04-01T18:27:17.454456Z",
     "shell.execute_reply.started": "2022-04-01T18:27:17.444485Z"
    },
    "id": "lrlBRTF-fV86"
   },
   "outputs": [],
   "source": [
    "class RecurrentMelSpectClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_classes=10,\n",
    "        window_length=1024,\n",
    "        hop_length=256,\n",
    "        hidden=256,\n",
    "        num_layers=2\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden = hidden\n",
    "        self.window_length = window_length\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(64, hidden, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden * num_layers, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        ### THE END OF YOUR CODE ###\n",
    "\n",
    "    def forward(self, x, lens) -> torch.Tensor:\n",
    "        h_0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden).to(device)\n",
    "        c_0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden).to(device)\n",
    "\n",
    "        # примените к получившемся последовательностям LSTM и возьмите hidden state\n",
    "\n",
    "        out, hidden_state = self.lstm(x.permute(0, 2, 1), (h_0, c_0))\n",
    "\n",
    "        # склейте hidden_state по слоям\n",
    "        # hidden_flattened.shape = (B, 2 * hidden_size * num_layers)\n",
    "        hidden_flattened = torch.reshape(hidden_state[0], (x.size(0), -1))\n",
    "\n",
    "        # примените полносвязную сеть и получим логиты классов\n",
    "        return self.final_mlp(hidden_flattened)  # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:17.456782Z",
     "iopub.status.busy": "2022-04-01T18:27:17.456357Z",
     "iopub.status.idle": "2022-04-01T18:27:17.488804Z",
     "shell.execute_reply": "2022-04-01T18:27:17.488089Z",
     "shell.execute_reply.started": "2022-04-01T18:27:17.456743Z"
    },
    "id": "a9Z4a-W6jBQX"
   },
   "outputs": [],
   "source": [
    "rnn_mel = RecurrentMelSpectClassifier()\n",
    "rnn_mel.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(rnn_mel.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:27:17.490600Z",
     "iopub.status.busy": "2022-04-01T18:27:17.490138Z",
     "iopub.status.idle": "2022-04-01T18:40:50.353270Z",
     "shell.execute_reply": "2022-04-01T18:40:50.352335Z",
     "shell.execute_reply.started": "2022-04-01T18:27:17.490536Z"
    },
    "id": "at77cRtEjt-r"
   },
   "outputs": [],
   "source": [
    "train_audio_clfr(rnn_mel, optim, train_dataloader, train_dataset.sr, \n",
    "                 data_transform=compute_log_melspectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpl1QhKlVkot"
   },
   "source": [
    "Посчитаем метрики на валидационном датасете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOXx3pG58cHm"
   },
   "source": [
    "**Task: to get 0.5 points, select hyperparameters and achieve an accuracy of the model above 0.8 on the validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:40:50.355582Z",
     "iopub.status.busy": "2022-04-01T18:40:50.354727Z",
     "iopub.status.idle": "2022-04-01T18:41:01.138132Z",
     "shell.execute_reply": "2022-04-01T18:41:01.137419Z",
     "shell.execute_reply.started": "2022-04-01T18:40:50.355519Z"
    },
    "id": "U2WPjo4tjyy0"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rnn_mel, val_dataloader, train_dataset.sr, device, \n",
    "                      data_transform=compute_log_melspectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od6HOpq8FXgH"
   },
   "source": [
    "## Task 5. Convolutional Network for Audio Classification Using Mel-Spectrograms (2 points)\n",
    "\n",
    "It is easy to observe that Mel-spectrograms exhibit distinct patterns—so much so that a trained human could _visually_ classify the object. \n",
    "\n",
    "This allows us to transform the audio classification task into an image classification problem.\n",
    "\n",
    "### Implement the following convolutional network:\n",
    "\n",
    "* 2x (Conv2d 3x3 @ 16, BatchNorm2d, ReLU)\n",
    "* MaxPool 2x2\n",
    "* 2x (Conv2d 3x3 @ 32, BatchNorm2d, ReLU)\n",
    "* MaxPool 2x2\n",
    "* 2x (Conv2d 3x3 @ 64, BatchNorm2d, ReLU)\n",
    "* MaxPool 2x2\n",
    "* (Conv2d 3x3 @ 128, BatchNorm2d, ReLU)\n",
    "* (Conv2d 2x2 @ 128, BatchNorm2d, ReLU)\n",
    "* Global MaxPool\n",
    "* Fully Connected 128, ReLU\n",
    "* Fully Connected 10\n",
    "\n",
    "**Tip:** A similar architecture was implemented in [**PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition**](https://arxiv.org/pdf/1912.10211.pdf). You can use this paper as a reference.\n",
    "\n",
    "**The implementation of this architecture is worth 1.5 points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:41:01.140113Z",
     "iopub.status.busy": "2022-04-01T18:41:01.139640Z",
     "iopub.status.idle": "2022-04-01T18:41:01.153510Z",
     "shell.execute_reply": "2022-04-01T18:41:01.152698Z",
     "shell.execute_reply.started": "2022-04-01T18:41:01.140074Z"
    },
    "id": "3BSb2SWxFXgI"
   },
   "outputs": [],
   "source": [
    "class CNN10(nn.Module):\n",
    "    def __init__(self, num_classes=10, hidden=16):\n",
    "        super().__init__()\n",
    "\n",
    "        ### YOUR CODE IS HERE ######\n",
    "        self.cnn_backbone = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2),\n",
    "          nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2),\n",
    "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2),\n",
    "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=128, out_channels=128, kernel_size=2),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        z = self.cnn_backbone(x[:, None, :, :])\n",
    "        z = torch.nn.functional.max_pool2d(z, kernel_size=z.size()[2:])[:, :, 0, 0]\n",
    "        return self.final_mlp(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:41:01.155469Z",
     "iopub.status.busy": "2022-04-01T18:41:01.154662Z",
     "iopub.status.idle": "2022-04-01T18:41:01.176712Z",
     "shell.execute_reply": "2022-04-01T18:41:01.176051Z",
     "shell.execute_reply.started": "2022-04-01T18:41:01.155426Z"
    },
    "id": "9mmkvk5nFXgK"
   },
   "outputs": [],
   "source": [
    "cnn = CNN10()\n",
    "cnn.to(device);\n",
    "\n",
    "optim = torch.optim.Adam(cnn.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:41:01.178520Z",
     "iopub.status.busy": "2022-04-01T18:41:01.178038Z",
     "iopub.status.idle": "2022-04-01T18:55:44.688147Z",
     "shell.execute_reply": "2022-04-01T18:55:44.687375Z",
     "shell.execute_reply.started": "2022-04-01T18:41:01.178484Z"
    },
    "id": "_-z39AbO8_3Z"
   },
   "outputs": [],
   "source": [
    "train_audio_clfr(cnn, optim, train_dataloader, train_dataset.sr, \n",
    "                 data_transform=compute_log_melspectrogram,\n",
    "                 num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM-B_VQZ-dJX"
   },
   "source": [
    "**Task: to get 0.5 points, select hyperparameters and achieve an accuracy of the model above 0.85 on the validation dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:55:44.690034Z",
     "iopub.status.busy": "2022-04-01T18:55:44.689593Z",
     "iopub.status.idle": "2022-04-01T18:55:52.245457Z",
     "shell.execute_reply": "2022-04-01T18:55:52.244785Z",
     "shell.execute_reply.started": "2022-04-01T18:55:44.689998Z"
    },
    "id": "PVbI7IAcQNjX"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnn, val_dataloader, train_dataset.sr, device, \n",
    "                      data_transform=compute_log_melspectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cu9fpMOikY6L"
   },
   "source": [
    "## Task 6. SpecAugment Data Augmentation (2 points)\n",
    "\n",
    "Audio datasets are typically quite small. Our dataset is a good example, with only 4,500 samples in the training set. Training deep networks with a large number of parameters on such datasets often leads to overfitting and a drop in validation and test metrics.\n",
    "\n",
    "To combat overfitting, data augmentation can be used. For Mel-spectrograms, a technique called **SpecAugment** was developed.\n",
    "\n",
    "### The core idea of SpecAugment:\n",
    "It zeroes out parts of the spectrogram along the time and frequency axes:\n",
    "\n",
    "1. Select several time intervals \\({[t^1_i, t^2_i]}\\) and set the spectrogram values \\(s[t^1_i : t^2_i, :]\\) to \\(v\\).\n",
    "2. Select several frequency intervals \\({[m^1_i, m^2_i]}\\) and set the spectrogram values \\(s[:, m^1_i : m^2_i]\\) to \\(v\\).\n",
    "\n",
    "### The value \\(v\\) can be:\n",
    "1. `'mean'`: the mean of the spectrogram\n",
    "2. `'min'`: the minimum of the spectrogram\n",
    "3. `'max'`: the maximum of the spectrogram\n",
    "4. `v`: a constant\n",
    "\n",
    "**Tip:** A detailed description of SpecAugment can be found here: [link](https://neurohive.io/ru/novosti/specaugment-novyj-metod-augmentacii-audiodannyh-ot-google-ai/), which you can use as a reference.\n",
    "\n",
    "![specaugment](https://neurohive.io/wp-content/uploads/2019/04/image6.png)\n",
    "\n",
    "### Task:\n",
    "Implement the **SpecAugment** augmentation.\n",
    "\n",
    "**The implementation of SpecAugment is worth 1.5 points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:55:52.246951Z",
     "iopub.status.busy": "2022-04-01T18:55:52.246587Z",
     "iopub.status.idle": "2022-04-01T18:55:52.263637Z",
     "shell.execute_reply": "2022-04-01T18:55:52.262981Z",
     "shell.execute_reply.started": "2022-04-01T18:55:52.246915Z"
    },
    "id": "lmfkTTrfFXgp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class SpectAugment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filling_value = \"mean\",\n",
    "        n_freq_masks = 2,\n",
    "        n_time_masks = 2,\n",
    "        max_freq = 10,\n",
    "        max_time = 50,\n",
    "    ):\n",
    "\n",
    "        self.filling_value = filling_value\n",
    "        self.n_freq_masks = n_freq_masks\n",
    "        self.n_time_masks = n_time_masks\n",
    "        self.max_freq = max_freq\n",
    "        self.max_time = max_time\n",
    "\n",
    "    def __call__(self, spect, lens):\n",
    "        ### YOUR CODE IS HERE ######\n",
    "        \n",
    "        torch.random.manual_seed(4)\n",
    "        \n",
    "        num_frequency_bins = spect.shape[1]\n",
    "        num_time_bins = spect.shape[2]\n",
    "\n",
    "        for i in range(self.n_freq_masks):\n",
    "            freq = random.randint(0, self.max_freq)\n",
    "            bounds_freq_1 = random.randint(0, num_frequency_bins - self.max_freq)\n",
    "            bounds_freq_2 = bounds_freq_1 + freq\n",
    "            if self.filling_value == \"mean\":\n",
    "                fill_values = torch.reshape((torch.mean(spect, axis=2)[:, bounds_freq_1:bounds_freq_2]).repeat_interleave(1497, dim=1), (32, bounds_freq_2-bounds_freq_1, 1497))\n",
    "            elif self.filling_value == \"min\":\n",
    "                fill_values = torch.reshape((torch.min(spect, axis=2)[:, bounds_freq_1:bounds_freq_2]).repeat_interleave(1497, dim=1), (32, bounds_freq_2-bounds_freq_1, 1497))\n",
    "            elif self.filling_value == \"max\":\n",
    "                fill_values = torch.reshape((torch.max(spect, axis=2)[:, bounds_freq_1:bounds_freq_2]).repeat_interleave(1497, dim=1), (32, bounds_freq_2-bounds_freq_1, 1497))\n",
    "            else:\n",
    "                # self.filling_value == \"constant\"\n",
    "                fill_values = self.constant\n",
    "\n",
    "            spect[:, bounds_freq_1 : bounds_freq_2, :] = fill_values\n",
    "    \n",
    "    \n",
    "        for j in range(self.n_time_masks):\n",
    "            time = random.randint(0, self.max_time)\n",
    "            bounds_time_1 = random.randint(0, num_time_bins - self.max_time)\n",
    "            bounds_time_2 = bounds_time_1 + time\n",
    "            if self.filling_value == \"mean\":\n",
    "                fill_values = torch.reshape((torch.mean(spect, axis=1)[:, bounds_time_1:bounds_time_2]).repeat_interleave(64, dim=1), (32, 64, bounds_time_2-bounds_time_1))\n",
    "            elif self.filling_value == \"min\":\n",
    "                fill_values = torch.reshape((torch.min(spect, axis=1)[:, bounds_time_1:bounds_time_2]).repeat_interleave(64, dim=1), (32, 64, bounds_time_2-bounds_time_1))\n",
    "            elif self.filling_value == \"max\":\n",
    "                fill_values = torch.reshape((torch.max(spect, axis=1)[:, bounds_time_1:bounds_time_2]).repeat_interleave(64, dim=1), (32, 64, bounds_time_2-bounds_time_1))\n",
    "            else:\n",
    "                # self.filling_value == \"constant\"\n",
    "                fill_values = self.constant\n",
    "\n",
    "            spect[:, :, bounds_time_1 : bounds_time_2] = fill_values\n",
    "    \n",
    "        return spect, lens // 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:55:52.266648Z",
     "iopub.status.busy": "2022-04-01T18:55:52.266297Z",
     "iopub.status.idle": "2022-04-01T18:55:52.670986Z",
     "shell.execute_reply": "2022-04-01T18:55:52.670242Z",
     "shell.execute_reply.started": "2022-04-01T18:55:52.266610Z"
    },
    "id": "GXaJi2nfqE4J"
   },
   "outputs": [],
   "source": [
    "# применим аугментацию к данным\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "x = batch[\"x\"].to(device)\n",
    "lens = batch[\"len\"].to(device)\n",
    "x_logmel, lens = compute_log_melspectrogram_reference(x, lens, sr=train_dataset.sr, device=device)\n",
    "x_logmel_augmented, lens = SpectAugment()(x_logmel, lens)\n",
    "\n",
    "# нарисуем спектрограмму до и после аугментации\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"Original log MelSpectrogram\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(x_logmel[0].cpu().numpy())\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"Augmented log MelSpectrogram\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(x_logmel_augmented[0].cpu().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:55:52.672701Z",
     "iopub.status.busy": "2022-04-01T18:55:52.672412Z",
     "iopub.status.idle": "2022-04-01T18:55:52.686791Z",
     "shell.execute_reply": "2022-04-01T18:55:52.686134Z",
     "shell.execute_reply.started": "2022-04-01T18:55:52.672663Z"
    },
    "id": "XdohpTw5l6Lk"
   },
   "outputs": [],
   "source": [
    "cnn = CNN10()\n",
    "cnn.to(device);\n",
    "\n",
    "optim = torch.optim.Adam(cnn.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T18:55:52.688320Z",
     "iopub.status.busy": "2022-04-01T18:55:52.687878Z",
     "iopub.status.idle": "2022-04-01T19:10:41.422830Z",
     "shell.execute_reply": "2022-04-01T19:10:41.421013Z",
     "shell.execute_reply.started": "2022-04-01T18:55:52.688281Z"
    },
    "id": "59YvirCHm_ye"
   },
   "outputs": [],
   "source": [
    "# обучим модель на данных с аугментациями\n",
    "train_audio_clfr(cnn, optim, train_dataloader, train_dataset.sr, \n",
    "                 data_transform=compute_log_melspectrogram,\n",
    "                 augmentation=SpectAugment(),\n",
    "                 num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI_0no5Z_OZv"
   },
   "source": [
    "**Task: to get 0.5 points, select the augmentation parameters and achieve an accuracy of the model above 0.9 on the validation dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T19:10:41.424514Z",
     "iopub.status.busy": "2022-04-01T19:10:41.424256Z",
     "iopub.status.idle": "2022-04-01T19:10:48.559017Z",
     "shell.execute_reply": "2022-04-01T19:10:48.558354Z",
     "shell.execute_reply.started": "2022-04-01T19:10:41.424478Z"
    },
    "id": "F7kgOAn-nHSU"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnn, val_dataloader, train_dataset.sr, device, \n",
    "                      data_transform=compute_log_melspectrogram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
