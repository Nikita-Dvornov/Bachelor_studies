{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 1,
    "id": "kr9vAeEQlRVG"
   },
   "source": [
    "# Homework 2. Classification of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 3,
    "id": "BxX49gLclRVJ"
   },
   "source": [
    "# Task 1: Image Classification (1 point).\n",
    "\n",
    "In this task, you will need to train an image classifier. We will be working with a dataset, the name of which will not be revealed. You can check the images available in the dataset by yourself. The dataset has 200 classes and around 5,000 images per class. The classes are numbered from 0 to 199. You can download the dataset [here](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "The dataset structure is simple — it has `train/` and `val/` directories, which contain the training and validation data. Inside `train/` and `val/`, there are subdirectories corresponding to image classes, and the images themselves are inside these subdirectories.\n",
    "\n",
    "## Task.\n",
    "\n",
    "You need to complete one of the two tasks:\n",
    "\n",
    "1) Achieve validation accuracy of **at least 0.44**. In this task, **using pretrained models and image resizing is forbidden**.\n",
    "\n",
    "2) Achieve validation accuracy of **at least 0.84**. In this task, resizing and using pretrained models is allowed.\n",
    "\n",
    "Write a brief report on your experiments. What worked and what didn’t? Why did you decide to do it that way? Be sure to provide links to external code if you are using it. Always reference articles / blog posts / StackOverflow questions / YouTube videos from machine learning creators / courses / tips from Uncle Vasya, and any other additional materials you used.\n",
    "\n",
    "Your code must pass all `assert` checks below.\n",
    "\n",
    "You must write the functions `train_one_epoch`, `train`, and `predict` according to the templates below (they largely repeat examples from the seminars). Pay special attention to the `predict` function: it should return a list of losses for all objects in the dataloader, a list of predicted classes for each object from the dataloader, and a list of true classes for each object in the dataloader (in exactly this order).\n",
    "\n",
    "**Using external data for training is strictly prohibited in both tasks. Also, training on the validation set is forbidden**.\n",
    "\n",
    "### Evaluation criteria:\n",
    "The evaluation is calculated by a simple formula: `min(10, 10 * Your accuracy / 0.44)` for the first task and `min(10, 10 * (Your accuracy - 0.5) / 0.34)` for the second. The result is rounded to one decimal place. If you complete both tasks, the maximum of the two scores will be taken.\n",
    "\n",
    "### Bonus:\n",
    "You will receive 5 bonus points if you complete both tasks with a score of 10 (a total of 15 points). Otherwise, the maximum of the two scores will be taken, and your bonus will be zero.\n",
    "\n",
    "### Tips and Recommendations:\n",
    "- You will likely need to Google a lot about classification and how to make it work. This is normal; everyone Googles. But remember, you must be ready to explain the code you used :)\n",
    "- Use augmentations. You can use the `torchvision.transforms` module or the [albumentations](https://github.com/albumentations-team/albumentations) library.\n",
    "- You can either train from scratch or fine-tune (depending on the task) models from `torchvision`.\n",
    "- We recommend writing a custom dataset class (or using the `ImageFolder` class), which returns images and their corresponding labels, and then creating functions for training based on the templates below. However, we do not require this. If this style is inconvenient, you can write the code in your preferred style. Keep in mind that excessive changes to the templates below will increase the number of questions about your code and increase the likelihood of being called for a defense :)\n",
    "- Validate. Track errors as early as possible to avoid wasting time.\n",
    "- To quickly debug the code, try training on a small portion of the dataset (say, 5-10 images, just to check if the code runs). Once you’ve debugged everything, proceed with training on the full dataset.\n",
    "- Make exactly one change to the model/augmentation/optimizer for each run to understand what influences the result.\n",
    "- Fix the random seed.\n",
    "- Start with simple models and gradually move to more complex ones. Training light models saves a lot of time.\n",
    "- Set a learning rate schedule. Reduce it when the validation loss stops decreasing.\n",
    "- We recommend using a GPU. If you don’t have one, use Google Colab. If you are uncomfortable using it constantly, write and debug all the code locally on the CPU, and then run the notebook in Colab. The author's solution reaches the required accuracy in Colab in 15 minutes of training.\n",
    "\n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": 4,
    "id": "LKcSNj4tlRVK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# You may add any imports you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YhYnjPnrrZck"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-wYAtG5sweWm"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "offj7JmwxRyB",
    "outputId": "70ec5af6-8eb5-4e90-8b54-2f4d1437d8be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UpElx6DhsFrW"
   },
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True) \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.default_rng(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uMpT2jA3sTwh"
   },
   "outputs": [],
   "source": [
    "def plot_history_of_train(train_history, title=\"Train Loss\"):\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label=\"train\", zorder=1)\n",
    "\n",
    "    plt.xlabel(\"train steps\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "D3MP9x-esiVF"
   },
   "outputs": [],
   "source": [
    "def plot_history_of_test(val_history, title=\"Val Loss\"):\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(val_history, label=\"test\", zorder=1)\n",
    "\n",
    "    plt.xlabel(\"test steps\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.dropbox.com/s/33l8lp62rmvtx40/dataset.zip?dl=1\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Saving the dataset to a file\n",
    "with open(\"dataset.zip\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Unzipping the dataset\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Image classification.ipynb', 'dataset', '.ipynb_checkpoints', 'dataset.zip', 'Introduction to Pytorch. Fully connected neural networks.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in the current directory\n",
    "extracted_files = os.listdir()\n",
    "print(extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset']\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = 'dataset'\n",
    "dataset_contents = os.listdir(dataset_folder)\n",
    "\n",
    "print(dataset_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'val']\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the 'dataset' folder\n",
    "dataset_path = os.path.join(dataset_folder, 'dataset')\n",
    "dataset_contents = os.listdir(dataset_path)\n",
    "\n",
    "print(dataset_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List contents of 'train' and 'val' folders\n",
    "train_folder = os.path.join(dataset_path, 'train')\n",
    "val_folder = os.path.join(dataset_path, 'val')\n",
    "\n",
    "train_contents = os.listdir(train_folder)\n",
    "val_contents = os.listdir(val_folder)\n",
    "\n",
    "# print(\"Train folder contents:\", train_contents)\n",
    "# print(\"Validation folder contents:\", val_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# !wget 'https://www.dropbox.com/s/33l8lp62rmvtx40/dataset.zip?dl=1' -O dataset.zip && unzip -q dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0Dn0tRgI8ejr"
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=\"dataset/dataset/train\", transform=torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor()]))\n",
    "val_dataset = ImageFolder(root=\"dataset/dataset/val\", transform=torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RytEDW0ylRVN"
   },
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": 5,
    "id": "QEdDQtHdlRVO"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_2GwbgWGnIeQ"
   },
   "outputs": [],
   "source": [
    "# calculate the average and standard deviation of the dataset data for its subsequent normalization (we do it by matches so as not to get an error filling RAM)\n",
    "# taken from the following YouTube video: https://youtu.be/z3kB3ISIPAg\n",
    "\n",
    "def get_mean_and_std(dataloader):\n",
    "  mean = 0.\n",
    "  std = 0.\n",
    "  all_images_count = 0\n",
    "  for images, _ in dataloader:\n",
    "    images_count_in_a_batch = images.size(0)\n",
    "    images = images.view(images_count_in_a_batch, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    all_images_count += images_count_in_a_batch\n",
    "  \n",
    "  mean /= all_images_count\n",
    "  std /= all_images_count\n",
    "\n",
    "  return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aKOgRPUUoPT3",
    "outputId": "fc26f568-6341-4e6c-ec58-8395fba0a846"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4802, 0.4481, 0.3975]), tensor([0.2302, 0.2265, 0.2262]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_and_std(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NCpcHihRo9GU",
    "outputId": "7360add5-886d-4c9c-bff2-a26e4e635c63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4824, 0.4495, 0.3981]), tensor([0.2301, 0.2264, 0.2261]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_and_std(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(hue=.06, saturation=.04),\n",
    "    transforms.RandomEqualize(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomGrayscale(p=0.15),\n",
    "    transforms.RandomRotation(20),  # Correct way to use resample\n",
    "    transforms.RandomAutocontrast(p=0.5),\n",
    "    transforms.RandomInvert(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4802, 0.4481, 0.3975), (0.2302, 0.2265, 0.2262))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(hue=.06, saturation=.04),\n",
    "    transforms.RandomEqualize(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomGrayscale(p=0.15),\n",
    "    transforms.RandomRotation(20),  # Correct way to use resample\n",
    "    transforms.RandomAutocontrast(p=0.5),\n",
    "    transforms.RandomInvert(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4824, 0.4495, 0.3981), (0.2301, 0.2264, 0.2261))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ZwnNxfM1gKbi"
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=\"dataset/dataset/train\", transform=train_transform)\n",
    "val_dataset = ImageFolder(root=\"dataset/dataset/val\", transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "yKDfpQaSbqOD"
   },
   "outputs": [],
   "source": [
    "set_random_seed(12)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, drop_last=True, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_id": 6,
    "id": "mrg4Yj0VlRVP",
    "outputId": "d0d6b208-e955-45f8-b152-2bb434f1eaf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "# Just very simple sanity checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "m7yemv_8C7WY",
    "outputId": "f74a10f1-f21b-4182-9004-9153263df7c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RlSlmyjlRVP"
   },
   "source": [
    "### Auxiliary functions, model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "1or0WLRKrk9J"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, criterion, optimizer, n_epochs=10, device=None, scheduler=None):\n",
    "\n",
    "    train_loss_log, train_acc_log = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch number %d' % (epoch + 1))\n",
    "        train_epoch_loss, train_epoch_true_hits = torch.empty(0), 0.0\n",
    "        total_amount = 0.0\n",
    "        model.train()\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total_amount += labels.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(images)\n",
    "            loss = criterion(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # log loss for the current epoch and the whole training history\n",
    "            train_epoch_loss = torch.cat((train_epoch_loss, loss.unsqueeze(0).cpu()))\n",
    "            train_loss_log.append(loss.item())\n",
    "            \n",
    "            # log accuracy for the current epoch and the whole training history\n",
    "            train_epoch_true_hits += (y_pred.argmax(1) == labels).sum().item()\n",
    "            train_acc_log.append((y_pred.argmax(1) == labels).sum().item())\n",
    "            \n",
    "        mean_loss = torch.mean(train_epoch_loss)\n",
    "        scheduler.step(mean_loss)\n",
    "        \n",
    "        plot_history_of_train(train_loss_log)\n",
    "\n",
    "        print(\"Train_epoch_loss:\", torch.mean(train_epoch_loss))\n",
    "        print(\"Train_epoch_accuracy:\", train_epoch_true_hits / total_amount)\n",
    "    \n",
    "    \n",
    "def predict(model, val_dataloder, criterion, n_epochs, device=None):\n",
    "\n",
    "    val_loss_log, val_acc_log = [], []\n",
    "  \n",
    "    model.eval()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch number %d' % (epoch + 1))\n",
    "        val_epoch_losses, val_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
    "        predicted_classes = torch.empty(0)\n",
    "        true_classes = torch.empty(0)                          \n",
    "        total_amount = 0.0\n",
    "        with torch.no_grad():                                        \n",
    "            for batch in tqdm.tqdm(val_dataloader): \n",
    "                images, labels = batch \n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device) # берем батч из вал лоадера\n",
    "                total_amount += labels.size(0)\n",
    "                true_classes = torch.cat((true_classes, labels.unsqueeze(0).cpu())) \n",
    "                y_pred = model(images)                        \n",
    "                loss = criterion(y_pred, labels)              \n",
    "                val_epoch_losses = torch.cat((val_epoch_losses, loss.unsqueeze(0).cpu()))\n",
    "                pred_classes = torch.argmax(y_pred, dim=-1)\n",
    "                predicted_classes = torch.cat((predicted_classes,  pred_classes.unsqueeze(0).cpu())) \n",
    "                val_epoch_true_hits = torch.cat((val_epoch_true_hits, (pred_classes == labels).sum().unsqueeze(0).cpu()))\n",
    "                val_loss_log.append(val_epoch_losses.mean())\n",
    "                val_acc_log.append((pred_classes == labels).sum().item())\n",
    "\n",
    "            print(\"Val loss:\", val_epoch_losses.mean().item())\n",
    "            print(\"Val accuracy:\", (val_epoch_true_hits.sum() / total_amount).item())\n",
    "    \n",
    "            plot_history_of_test(val_loss_log)\n",
    "\n",
    "            val_epoch_losses = torch.reshape(val_epoch_losses, (-1, 1))\n",
    "            predicted_classes = torch.reshape(predicted_classes, (-1, 1))\n",
    "            true_classes = torch.reshape(true_classes, (-1, 1))\n",
    "    return val_epoch_losses, predicted_classes, true_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxR3gfcilRVW"
   },
   "source": [
    "### Model training, experiment launches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_id": 8,
    "id": "JXFJ6oS8lRVX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model_unpretrained = resnet18(pretrained=False)\n",
    "model_unpretrained.fc = nn.Linear(512, 200)\n",
    "model_unpretrained.to(device)\n",
    "optimizer = torch.optim.Adam(model_unpretrained.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unpretrained = torchvision.models.densenet161(pretrained=False)\n",
    "model_unpretrained.classifier = nn.Linear(2208, 200)\n",
    "model_unpretrained.to(device)\n",
    "optimizer = torch.optim.Adam(model_unpretrained.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "5n3QlCm_k8Vb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/nikitadvornov/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 53.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_pretrained = resnet18(pretrained=True)\n",
    "model_pretrained.to(device)\n",
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "model_pretrained.fc = nn.Linear(512, 200)\n",
    "optimizer = torch.optim.Adam([p for p in model_pretrained.parameters() if p.requires_grad], lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "IAkVXmJxaa3x",
    "outputId": "3e41824c-1942-4e7c-f2fa-905347f8af4d"
   },
   "outputs": [],
   "source": [
    "train(model_unpretrained, train_dataloader, criterion, optimizer, 8, device, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T19:45:33.944848Z",
     "iopub.status.busy": "2021-11-21T19:45:33.944647Z",
     "iopub.status.idle": "2021-11-21T19:47:50.530271Z",
     "shell.execute_reply": "2021-11-21T19:47:50.529489Z",
     "shell.execute_reply.started": "2021-11-21T19:45:33.944823Z"
    },
    "id": "dpG8lz70wI55"
   },
   "outputs": [],
   "source": [
    "predict(model_unpretrained, val_dataloader, criterion, 10, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 9,
    "id": "CesoOl6BlRVY"
   },
   "source": [
    "A simple test to check the correctness of the written code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "cell_id": 10,
    "id": "B_LB2jn6lRVY"
   },
   "outputs": [],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model_unpretrained, val_dataloader, criterion, 2, device)\n",
    "assert len(predicted_labels) == len(val_dataset) - 16       # 16 - the size of the last (dropped) batch (len(val_dataset) - len(predicted_labels) = 16)\n",
    "accuracy = accuracy_score(predicted_labels, true_labels)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImVW8_EXlRVZ"
   },
   "source": [
    "### Checking the received accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 13,
    "id": "FmR-elhJlRVZ"
   },
   "source": [
    "After all the experiments that you have done, choose the best of your models, implement and run the `evaluate` function. This function should take a model and a dataloader with validation data as input and return the accuracy calculated on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cell_id": 14,
    "id": "3TGH0EFalRVb"
   },
   "outputs": [],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model_unpretrained, val_dataloader, criterion, 1, device)\n",
    "assert len(predicted_labels) == len(val_dataset) - 16\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(10, 10 * accuracy / 0.44)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 15,
    "id": "pT8vfPSolRVb"
   },
   "source": [
    "### Experiments Report\n",
    "\n",
    "I've tried different models from torch vision.models. The most successful model turned out to be the densenet16, which is slightly better than the usual reznet. I also tried different step lengths and schedulers, such as, for example, lr_scheduler.Exponential LR and lr_scheduler.ReduceLROnPlateau, settling on the latter as the most well-behaved. The best model in the end is as follows: touch vision.models.densenet161(pretrained=False) with the parameters written in the corresponding cell above (accuracy = 0.417 with 8 training epochs). The code for the second task was also written, and I didn't use anything more than the usual resnet, so the quality there is clearly no better than in the first task (in relative terms, of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
